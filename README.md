# Data Processing and Analysis---Air-Conditioner
In this project, I build an end-to-end data pipeline using AWS, Databricks, Pyspark and Snowflake. The pipeline involves retrieving data from an AWS S3 bucket, performing data cleansing and transformation, and subsequently loading the refined data into Snowflake for analysis.
<img width="723" alt="databricks" src="https://github.com/jaylai28/Data-Analysis---Air-Conditioner/assets/69461406/98e7a10e-a610-4cf8-9f8c-14ca78e30be6">


Steps in the Pipeline:
1. Retrieving Data from AWS S3:
  - Data is fetched from an AWS S3 bucket, where it's stored securely and reliably.
2. Data Processing and Cleansing:
  - The retrieved data undergoes thorough processing to remove duplicates, handle missing values, and correct data types.
3. Transformation using Apache Spark APIs in Databricks:
  - Apache Spark's powerful APIs are used to reshape and refine the data, which is then used for in-depth analysis.
4. Loading Data into Snowflake for Analysis:
  - The cleansed and transformed data is loaded into Snowflake's data warehouse, for storage and retrieval.
5. Uncover Insights through Analysis:
  - With the data now available in Snowflake, I can derive insights, run SQL queries, create visualizations, and perform diverse analyses to gain insights.

